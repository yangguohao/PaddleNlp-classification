{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**该文章的基本内容框架与大神[炼丹师](https://aistudio.baidu.com/aistudio/projectdetail/2311230?channelType=0&channel=0)的这个内容基本一致。**\n",
    "\n",
    "**这里主要分享一些我认为比较重要的，可以提升分数的方法。虽然我尝试了一些，效果都不太好，但是希望能抛砖引玉，仅供参考。**\n",
    "\n",
    "# 1.数据分类的不均衡\n",
    "\n",
    "14个类别中最多的有14万的数据，最少的只有3000。然而我之前采用了过采样或欠采样的方法，效果一般。\n",
    "\n",
    "\n",
    "过采样可能会导致某些少数据量的类别过拟合，而欠采样又会浪费大量的其他类别的数据。\n",
    "\n",
    "\n",
    "解决的方法还有\n",
    "\n",
    "（1）修改分类阈值：直接使用类别分布不均衡的数据训练分类器，会使得模型在预测时更偏向于多数类，所以不再以0.5为分类阈值，而是针对少数类在模型仅有较小把握时就将样本归为少数类。\n",
    "\n",
    "（2）代价敏感学习：比如LR算法中设置class_weight参数。\n",
    "\n",
    "# 2.r-drop正则化\n",
    "\n",
    "看到相关文献报道，在很多任务上使用r-drop正则化后能明显提升模型的能力，是否可以在文本分类中使用r-drop以提高分数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "train = pd.read_table('train.txt', sep='\\t',header=None)  # 训练集\r\n",
    "dev = pd.read_table('dev.txt', sep='\\t',header=None)      # 验证集\r\n",
    "test = pd.read_table('test.txt', sep='\\t',header=None)    # 测试集\r\n",
    "\r\n",
    "# 添加列名便于对数据进行更好处理\r\n",
    "train.columns = [\"text_a\",'label']\r\n",
    "dev.columns = [\"text_a\",'label']\r\n",
    "test.columns = [\"text_a\"]\r\n",
    "train.to_csv('train.csv', sep='\\t', index=False)  # 保存训练集，格式为text_a,label\r\n",
    "dev.to_csv('dev.csv', sep='\\t', index=False)      # 保存验证集，格式为text_a,label\r\n",
    "test.to_csv('test.csv', sep='\\t', index=False)    # 保存测试集，格式为text_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import collections\r\n",
    "from functools import partial\r\n",
    "import random\r\n",
    "import time\r\n",
    "import inspect\r\n",
    "import importlib\r\n",
    "from tqdm import tqdm\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddle.io import IterableDataset\r\n",
    "from paddle.utils.download import get_path_from_url\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "from paddle.dataset.common import md5file\r\n",
    "from paddlenlp.datasets import DatasetBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-11-04 15:46:00,861] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\n",
      "W1104 15:46:00.863762  2106 device_context.cc:404] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.0, Runtime API Version: 10.1\n",
      "W1104 15:46:00.868746  2106 device_context.cc:422] device: 0, cuDNN Version: 7.6.\n",
      "[2021-11-04 15:46:05,453] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-1.0/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"ernie-1.0\"\r\n",
    "# 只需指定想要使用的模型名称和文本分类的类别数即可完成Fine-tune网络定义，通过在预训练模型后拼接上一个全连接网络（Full Connected）进行分类\r\n",
    "model = ppnlp.transformers.ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=14,dropout=0.3) # 此次分类任务为14分类任务，故num_classes设置为14\r\n",
    "# 定义模型对应的tokenizer，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。需注意tokenizer类要与选择的模型相对应，具体可以查看PaddleNLP相关文档\r\n",
    "tokenizer = ppnlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.NewsData'>\n"
     ]
    }
   ],
   "source": [
    "label_list=list(train.label.unique())\r\n",
    "class NewsData(DatasetBuilder):\r\n",
    "    SPLITS = {\r\n",
    "        'train': 'train.csv',  # 训练集\r\n",
    "        'dev': 'dev.csv',      # 验证集\r\n",
    "    }\r\n",
    "\r\n",
    "    def _get_data(self, mode, **kwargs):\r\n",
    "        filename = self.SPLITS[mode]\r\n",
    "        return filename\r\n",
    "\r\n",
    "    def _read(self, filename):\r\n",
    "        \"\"\"读取数据\"\"\"\r\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\r\n",
    "            head = None\r\n",
    "            for line in f:\r\n",
    "                data = line.strip().split(\"\\t\")    # 以'\\t'分隔各列\r\n",
    "                if not head:\r\n",
    "                    head = data\r\n",
    "                else:\r\n",
    "                    text_a, label = data\r\n",
    "                    yield {\"text_a\": text_a, \"label\": label}  # 此次设置数据的格式为：text_a,label，可以根据具体情况进行修改\r\n",
    "\r\n",
    "    def get_labels(self):\r\n",
    "        return label_list   # 类别标签\r\n",
    "def load_dataset(name=None,\r\n",
    "                 data_files=None,\r\n",
    "                 splits=None,\r\n",
    "                 lazy=None,\r\n",
    "                 **kwargs):\r\n",
    "   \r\n",
    "    reader_cls = NewsData  # 加载定义的数据集格式\r\n",
    "    print(reader_cls)\r\n",
    "    if not name:\r\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\r\n",
    "    else:\r\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\r\n",
    "\r\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\r\n",
    "    return datasets\r\n",
    "train_ds, dev_ds = load_dataset(splits=[\"train\", \"dev\"])\r\n",
    "def convert_example(example, tokenizer, max_seq_length=128, is_test=False):\r\n",
    "    qtconcat = example[\"text_a\"]\r\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)  # tokenizer处理为模型可接受的格式 \r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        return input_ids, token_type_ids\r\n",
    "def create_dataloader(dataset,\r\n",
    "                      mode='train',\r\n",
    "                      batch_size=1,\r\n",
    "                      batchify_fn=None,\r\n",
    "                      trans_fn=None):\r\n",
    "    if trans_fn:\r\n",
    "        dataset = dataset.map(trans_fn)\r\n",
    "\r\n",
    "    shuffle = True if mode == 'train' else False\r\n",
    "    # 训练数据集随机打乱，测试数据集不打乱\r\n",
    "    if mode == 'train':\r\n",
    "        batch_sampler = paddle.io.DistributedBatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "    else:\r\n",
    "        batch_sampler = paddle.io.BatchSampler(\r\n",
    "            dataset, batch_size=batch_size, shuffle=shuffle)\r\n",
    "\r\n",
    "    return paddle.io.DataLoader(\r\n",
    "        dataset=dataset,\r\n",
    "        batch_sampler=batch_sampler,\r\n",
    "        collate_fn=batchify_fn,\r\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text_a': '网民市民集体幻想中奖后如果你中了9000万怎么办', 'label': 11}, {'text_a': 'PVC期货有望5月挂牌', 'label': 7}, {'text_a': '午时三刻新作《幻神录―宿命情缘》', 'label': 10}, {'text_a': '欧司朗LLFY网络提供一站式照明解决方案', 'label': 6}, {'text_a': '试探北京楼市向何方：排不完的队\\u3000涨不够的价', 'label': 8}, {'text_a': '个性测试：测你的生活无趣指数(图)', 'label': 12}, {'text_a': '浙江女排战前遇下马威天津球迷微博发出“威胁”', 'label': 1}, {'text_a': '澳央行行长重申未来需要加息', 'label': 3}, {'text_a': '海地总统选举第二轮投票将推迟举行', 'label': 2}, {'text_a': '苹果中国学生机开卖最高优惠1600元', 'label': 0}, {'text_a': '黄金分割位有支撑短期股指有望止跌企稳', 'label': 3}, {'text_a': '英国政府建议各部门每天发布2-10条微博客', 'label': 2}, {'text_a': '网游“吸金”新玩法', 'label': 0}, {'text_a': '短讯-阿森纳之王表态不回欧洲亨利在欧洲已赢得一切', 'label': 1}, {'text_a': '美男子因不满银行一怒推平豪宅', 'label': 8}, {'text_a': '刘孜喜得贵子不退出娱乐圈儿子名字叫NEMO', 'label': 4}, {'text_a': '智能图像调节索尼46寸液晶再次降价', 'label': 0}, {'text_a': '特别奉献：08-09赛季德甲威廉希尔-SSP赔率回顾', 'label': 11}, {'text_a': '空姐利用反恐技能智斗绑匪助破抢劫杀人大案', 'label': 9}, {'text_a': '中国人寿母公司减持金隅300万股', 'label': 3}]\n"
     ]
    }
   ],
   "source": [
    "print(dev_ds.data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 参数设置：\r\n",
    "# 批处理大小，显存如若不足的话可以适当改小该值  \r\n",
    "batch_size = 256\r\n",
    "# 文本序列最大截断长度，需要根据文本具体长度进行确定，最长不超过512。 通过文本长度分析可以看出文本长度最大为48，故此处设置为48\r\n",
    "max_seq_length = 48\r\n",
    "# 将数据处理成模型可读入的数据格式\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "\r\n",
    "# 训练集迭代器\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "\r\n",
    "# 验证集迭代器\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RDropLoss(nn.Layer):\r\n",
    "    \"\"\"\r\n",
    "    R-Drop Loss implementation\r\n",
    "    For more information about R-drop please refer to this paper: https://arxiv.org/abs/2106.14448\r\n",
    "    Original implementation please refer to this code: https://github.com/dropreg/R-Drop\r\n",
    "\r\n",
    "    Args:\r\n",
    "        reduction(str, optional):\r\n",
    "            Indicate how to average the loss, the candicates are ``'none'``,``'batchmean'``,``'mean'``,``'sum'``.\r\n",
    "            If `reduction` is ``'mean'``, the reduced mean loss is returned;\r\n",
    "            If `reduction` is ``'batchmean'``, the sum loss divided by batch size is returned;\r\n",
    "            If `reduction` is ``'sum'``, the reduced sum loss is returned;\r\n",
    "            If `reduction` is ``'none'``, no reduction will be applied.\r\n",
    "            Defaults to ``'none'``.\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, reduction='none'):\r\n",
    "        super(RDropLoss, self).__init__()\r\n",
    "        if reduction not in ['sum', 'mean', 'none', 'batchmean']:\r\n",
    "            raise ValueError(\r\n",
    "                \"'reduction' in 'RDropLoss' should be 'sum', 'mean' 'batchmean', or 'none', \"\r\n",
    "                \"but received {}.\".format(reduction))\r\n",
    "        self.reduction = reduction\r\n",
    "\r\n",
    "    def forward(self, p, q, pad_mask=None):\r\n",
    "        \"\"\"\r\n",
    "        Args:\r\n",
    "            p(Tensor): the first forward logits of training examples.\r\n",
    "            q(Tensor): the second forward logits of training examples.\r\n",
    "            pad_mask(Tensor, optional): The Tensor containing the binary mask to index with, it's data type is bool.\r\n",
    "\r\n",
    "        Returns:\r\n",
    "            Tensor: Returns tensor `loss`, the rdrop loss of p and q.\r\n",
    "        \"\"\"\r\n",
    "        p_loss = F.kl_div(\r\n",
    "            F.log_softmax(\r\n",
    "                p, axis=-1),\r\n",
    "            F.softmax(\r\n",
    "                q, axis=-1),\r\n",
    "            reduction=self.reduction)\r\n",
    "        q_loss = F.kl_div(\r\n",
    "            F.log_softmax(\r\n",
    "                q, axis=-1),\r\n",
    "            F.softmax(\r\n",
    "                p, axis=-1),\r\n",
    "            reduction=self.reduction)\r\n",
    "\r\n",
    "        # pad_mask is for seq-level tasks\r\n",
    "        if pad_mask is not None:\r\n",
    "            p_loss = paddle.masked_select(p_loss, pad_mask)\r\n",
    "            q_loss = paddle.masked_select(q_loss, pad_mask)\r\n",
    "\r\n",
    "        # You can choose whether to use function \"sum\" and \"mean\" depending on your task\r\n",
    "        p_loss = p_loss.sum()\r\n",
    "        q_loss = q_loss.sum()\r\n",
    "        loss = (p_loss + q_loss) / 2\r\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[14], dtype=float64, place=CUDAPlace(0), stop_gradient=True,\n",
      "       [0.36653729 , 0.45379879 , 0.94663300 , 0.38678983 , 0.64470694 , 1.42405025 , 1.83264896 , 1.60974958 , 2.97854966 , 1.17443305 , 2.45021556 , 7.86938925 , 16.68672107, 4.46708183 ])\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "\r\n",
    "# 定义训练配置参数：\r\n",
    "# 定义训练过程中的最大学习率\r\n",
    "learning_rate = 5e-5\r\n",
    "# 训练轮次\r\n",
    "epochs = 10\r\n",
    "# 学习率预热比例\r\n",
    "warmup_proportion = 0.1\r\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\r\n",
    "\r\n",
    "# AdamW优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ])\r\n",
    "from collections import Counter\r\n",
    "c = Counter(train['label'])\r\n",
    "total = sum(c.values())\r\n",
    "weight = paddle.to_tensor([total/(len(label_list)*c[x]) for x in label_list],dtype='float64')\r\n",
    "print(weight)\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(weight=weight)  # 交叉熵损失函数\r\n",
    "rdrop_loss = RDropLoss(reduction='batchmean')\r\n",
    "metric = paddle.metric.Accuracy()              # accuracy评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['科技', '体育', '时政', '股票', '娱乐', '教育', '家居', '财经', '房产', '社会', '游戏', '彩票', '星座', '时尚']\n"
     ]
    }
   ],
   "source": [
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型训练验证评估函数\r\n",
    "@paddle.no_grad()\r\n",
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))  # 输出验证集上评估效果\r\n",
    "    model.train()\r\n",
    "    metric.reset()\r\n",
    "    return accu  # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "save_dir = \"checkpoint\"\r\n",
    "if not  os.path.exists(save_dir):\r\n",
    "    os.makedirs(save_dir)\r\n",
    "rdrop_coef = 4\r\n",
    "pre_accu=0\r\n",
    "accu=0\r\n",
    "global_step = 0\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        if rdrop_coef>0:\r\n",
    "            logits_2 = model(\r\n",
    "                input_ids, segment_ids)\r\n",
    "            ce_loss = (criterion(logits, labels) + criterion(logits_2, labels)) * 0.5\r\n",
    "            kl_loss = rdrop_loss(logits, logits_2)\r\n",
    "            loss = ce_loss + kl_loss * rdrop_coef\r\n",
    "        else:\r\n",
    "            loss = criterion(logits, labels)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "        \r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0 :\r\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        lr_scheduler.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "    # 每轮结束对验证集进行评估\r\n",
    "    accu = evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "    print(accu)\r\n",
    "    if accu > pre_accu:\r\n",
    "        # 保存较上一轮效果更优的模型参数\r\n",
    "        save_param_path = os.path.join(save_dir, 'model_state.pdparams')  # 保存模型参数\r\n",
    "        paddle.save(model.state_dict(), save_param_path)\r\n",
    "        pre_accu=accu\r\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from checkpoint/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "import paddle\r\n",
    "\r\n",
    "params_path = 'checkpoint/model_state.pdparams'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义模型预测函数\r\n",
    "def predict(model, data, tokenizer, label_map, batch_size=1):\r\n",
    "    examples = []\r\n",
    "    # 将输入数据（list格式）处理为模型可接受的格式\r\n",
    "    for text in data:\r\n",
    "        input_ids, segment_ids = convert_example(\r\n",
    "            text,\r\n",
    "            tokenizer,\r\n",
    "            max_seq_length=128,\r\n",
    "            is_test=True)\r\n",
    "        examples.append((input_ids, segment_ids))\r\n",
    "\r\n",
    "    batchify_fn = lambda samples, fn=Tuple(\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\r\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\r\n",
    "    ): fn(samples)\r\n",
    "\r\n",
    "    # Seperates data into some batches.\r\n",
    "    batches = []\r\n",
    "    one_batch = []\r\n",
    "    for example in examples:\r\n",
    "        one_batch.append(example)\r\n",
    "        if len(one_batch) == batch_size:\r\n",
    "            batches.append(one_batch)\r\n",
    "            one_batch = []\r\n",
    "    if one_batch:\r\n",
    "        # The last batch whose size is less than the config batch_size setting.\r\n",
    "        batches.append(one_batch)\r\n",
    "\r\n",
    "    results = []\r\n",
    "    model.eval()\r\n",
    "    for batch in batches:\r\n",
    "        input_ids, segment_ids = batchify_fn(batch)\r\n",
    "        input_ids = paddle.to_tensor(input_ids)\r\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "        idx = idx.tolist()\r\n",
    "        labels = [label_map[i] for i in idx]\r\n",
    "        results.extend(labels)\r\n",
    "    return results  # 返回预测结果\r\n",
    "label_list=list(train.label.unique())\r\n",
    "label_map = { \r\n",
    "    idx: label_text for idx, label_text in enumerate(label_list)\r\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('./test.csv',sep='\\t')  \r\n",
    "\r\n",
    "# 定义对数据的预处理函数,处理为模型输入指定list格式\r\n",
    "def preprocess_prediction_data(data):\r\n",
    "    examples = []\r\n",
    "    for text_a in data:\r\n",
    "        examples.append({\"text_a\": text_a})\r\n",
    "    return examples\r\n",
    "\r\n",
    "# 对测试集数据进行格式处理\r\n",
    "data1 = list(test.text_a)\r\n",
    "examples = preprocess_prediction_data(data1)\r\n",
    "\r\n",
    "# 对测试集进行预测\r\n",
    "results = predict(model, examples, tokenizer, label_map, batch_size=16)   \r\n",
    "\r\n",
    "# 将list格式的预测结果存储为txt文件，提交格式要求：每行一个类别\r\n",
    "def write_results(labels, file_path):\r\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as f:\r\n",
    "        f.writelines(\"\\n\".join(labels))\r\n",
    "\r\n",
    "write_results(results, \"./result.txt\")\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: result.txt (deflated 89%)\n"
     ]
    }
   ],
   "source": [
    "# 因格式要求为zip，故需要将结果文件压缩为submission.zip提交文件\r\n",
    "!zip 'submission.zip' 'result.txt'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
